# Coursera Content-Based Course Recommender

A GitHub-ready **content-based recommendation system** for Coursera-style datasets that only include **course metadata** (no user interactions). It provides:

- ðŸ”Ž TF-IDF & (optional) Sentence-BERT embeddings
- ðŸ¤ Course-to-course similarity search
- ðŸ§© K-Means clustering + 2D visualization (UMAP)
- ðŸ–¥ï¸ Streamlit app for demo (search by text or find similar courses)
- âš™ï¸ Config-driven column mapping (robust to unknown schema)

> Works out-of-the-box with `data/Coursera.csv`. If your column names differ, tweak `config/config.yaml` and rerun.

## Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py         # Streamlit UI
â”œâ”€â”€ artifacts/                   # Saved models & vectors
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Column names & settings
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Coursera.csv             # Your dataset (not committed by default)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA_guide.md             # Lightweight EDA guide
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py              # Orchestrates end-to-end build
â”‚   â”œâ”€â”€ fe_text.py               # Text cleaning & feature building
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ similarity.py        # Similarity search (TF-IDF or SBERT)
â”‚   â”‚   â””â”€â”€ clustering.py        # KMeans + UMAP
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ io.py                # IO helpers
â”‚       â””â”€â”€ config.py            # Config loader
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ smoke_test.py            # Quick pipeline sanity check
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Quickstart

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Check/edit config**
   - Open `config/config.yaml`, confirm your column names (title, description, skills, etc.)

3. **Build artifacts (vectors, clusters)**
   ```bash
   python -m src.pipeline --mode build
   ```

4. **Run the app**
   ```bash
   streamlit run app/streamlit_app.py
   ```

## What it does

- Cleans & concatenates text fields â†’ builds `corpus`
- Creates TF-IDF vectors (default) or Sentence-BERT embeddings (set `use_sbert: true`)
- Saves `artifacts/`:
  - `tfidf_vectorizer.pkl`, `X_tfidf.npz` (or `sbert_model_name.txt`, `X_sbert.npy`)
  - `kmeans.pkl`, `umap_embedding.npy`
  - `courses.parquet` (cleaned dataset + ids)
- Streamlit UI to:
  - Search by free text
  - Pick an existing course and get **Top-N similar** courses
  - Explore clusters + 2D map

## Notes

- If you don't have GPU, keep `use_sbert: false` for speed.
- Data is ignored by Git; upload your CSV or document how to obtain it.
- Replace Coursera.csv with your own and update config accordingly.

## Dataset 
- using dataset from kaggle : Multi-Platform Online Courses Dataset
- Dataset URL : https://www.kaggle.com/datasets/everydaycodings/multi-platform-online-courses-dataset
- Only coursera data in project operation
